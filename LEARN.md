- GPU 메모리가 신이다. 
    - 시스템 메모리를 vram 대신 쓸 수 있는 방법이 있기는 한데 이걸 쓰면 PCIE 속도도 충분하지 못하다는걸 실감할 수 있다. (학습 한번하는데 엄청난 시간이 낭비됨)
- 로컬 SSD 에 데이터셋을 저장하는 것도 일이다. 
    - 게임설치파일 다 날렸음
    - docker 캐싱된 레이어 파일 용량이 매우 크다
    - 각종 컴파일러의 캐시도 용량이 크다
    - .git 폴더는 생각보다 용량이 크다.
- 모델은 생각보다 안어렵다 
    - 어디까지나 ***생각보단*** 안어렵다. (어렵다)
    - 381M 모델 학습에도 이것저것 더하면 학습에는 12GB 소모된다.
    - 모델이 커지면 float16을 써야한다.
- 학습 데이터는 생각보다 다루기 어렵다
    - 학습 데이터셋을 인터넷에서 다운로드 받는 것도 일이다. (데이터셋 하나가 24GB인 경우도 있음)
    - 학습 데이터를 통째로 메모리에 로드하는 건 불가능하다. (DMA가 이래서 필요하구나)
- 모델의 특이점
    - Transformer 구조에서는 토큰의 순서를 모델이 알 수 없어 RoPE와 같은 위치 임베딩 방식이 필요하다.
    - 모델을 한번 실행할 때 Batch 라는 개념이 있어 한 단계 모델 실행시에 하나의 유저의 입력을 처리하는 것이 아니라 여러 유저의 문장을 한번에 계산할 수 있다.
    - 별게 다있음 RMSNorm, RoPE, GQA, SwiGLU
    - 하지만 생각보다 (비교적) 최신 모델들도 옛날 transformer 구조에서 크게 다르지는 않다. (당연하지만)
- 그외 배운 것
    - 전기요금은 생각보다 비싸다
    - 하지만 난방비용은 아낄 수 있다
    - 집이 건조하다
    - 생각보다 선풍기가 PC 열관리에 효과적이다.

