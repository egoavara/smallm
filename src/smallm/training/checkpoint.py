"""Checkpoint management utilities."""

import re
import torch
from pathlib import Path
from typing import List, Tuple, Optional


class CheckpointManager:
    """ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ í´ë˜ìŠ¤ - ìµœê³  ì„±ëŠ¥ Nê°œë§Œ ìœ ì§€."""

    def __init__(
        self,
        checkpoint_dir: str,
        max_checkpoints: int = 5,
        save_best: bool = True,
        device: str = "cpu",
    ):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.max_checkpoints = max_checkpoints
        self.save_best = save_best
        self.device = device
        self.best_loss = float("inf")

    def get_best_path(self) -> Path:
        """best.pt ê²½ë¡œ ë°˜í™˜."""
        return self.checkpoint_dir / "best.pt"

    def get_checkpoint_path(self, step: int, loss: float) -> Path:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ ìƒì„± (stepê³¼ loss í¬í•¨)."""
        return self.checkpoint_dir / f"step_{step:06d}_loss_{loss:.4f}.pt"

    def list_checkpoints(self) -> List[Tuple[Path, int, float]]:
        """ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ë°˜í™˜ (path, step, loss) - loss ê¸°ì¤€ ì •ë ¬."""
        checkpoints = []
        pattern = re.compile(r"step_(\d+)_loss_([\d.]+)\.pt")

        for f in self.checkpoint_dir.glob("step_*.pt"):
            match = pattern.match(f.name)
            if match:
                step = int(match.group(1))
                loss = float(match.group(2))
                checkpoints.append((f, step, loss))

        # loss ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (ë‚®ì€ lossê°€ ë” ì¢‹ìŒ)
        checkpoints.sort(key=lambda x: x[2])
        return checkpoints

    def cleanup_old_checkpoints(self):
        """max_checkpoints ê°œìˆ˜ë¥¼ ì´ˆê³¼í•˜ëŠ” ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ."""
        checkpoints = self.list_checkpoints()

        if len(checkpoints) > self.max_checkpoints:
            # lossê°€ ë†’ì€ (ì„±ëŠ¥ì´ ë‚˜ìœ) ì²´í¬í¬ì¸íŠ¸ë“¤ ì‚­ì œ
            to_delete = checkpoints[self.max_checkpoints:]
            for path, step, loss in to_delete:
                path.unlink()
                print(f"   ğŸ—‘ï¸  Deleted: {path.name}")

    def save(
        self,
        step: int,
        loss: float,
        model: torch.nn.Module,
        optimizer: torch.optim.Optimizer,
        loss_history: list,
    ) -> str:
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë° ê´€ë¦¬."""
        checkpoint = {
            "step": step,
            "loss": loss,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "loss_history": loss_history,
            "model_config": {
                "n_layers": model.config.n_layers,
                "n_heads": model.config.n_heads,
                "n_kv_heads": model.config.n_kv_heads,
                "d_model": model.config.d_model,
                "d_ff": model.config.d_ff,
                "vocab_size": model.config.vocab_size,
                "max_seq_len": model.config.max_seq_len,
            },
        }

        saved_paths = []

        # best.pt ì €ì¥ (í˜„ì¬ lossê°€ bestë³´ë‹¤ ë‚®ìœ¼ë©´)
        if self.save_best and loss < self.best_loss:
            self.best_loss = loss
            best_path = self.get_best_path()
            torch.save(checkpoint, best_path)
            saved_paths.append(f"best.pt (loss: {loss:.4f})")

        # ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        ckpt_path = self.get_checkpoint_path(step, loss)
        torch.save(checkpoint, ckpt_path)
        saved_paths.append(ckpt_path.name)

        # ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
        self.cleanup_old_checkpoints()

        return ", ".join(saved_paths)

    def load_best(
        self, model: torch.nn.Module, optimizer: Optional[torch.optim.Optimizer] = None
    ) -> Tuple[Optional[int], list]:
        """best.pt ë¡œë“œ. ì„±ê³µ ì‹œ (step, loss_history) ë°˜í™˜, ì—†ìœ¼ë©´ (None, [])."""
        best_path = self.get_best_path()
        if best_path.exists():
            checkpoint = torch.load(best_path, map_location=self.device)
            model.load_state_dict(checkpoint["model_state_dict"])
            if optimizer is not None:
                optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
            step = checkpoint["step"]
            loss = checkpoint.get("loss", float("inf"))
            self.best_loss = loss
            print(f"Loaded best.pt (step: {step}, loss: {loss:.4f})")
            return step, checkpoint.get("loss_history", [])
        return None, []

    def load_checkpoint(
        self, path: str, model: torch.nn.Module, optimizer: Optional[torch.optim.Optimizer] = None
    ) -> Tuple[int, list]:
        """íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ."""
        checkpoint = torch.load(path, map_location=self.device)
        model.load_state_dict(checkpoint["model_state_dict"])
        if optimizer is not None:
            optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        step = checkpoint["step"]
        loss = checkpoint.get("loss", float("inf"))
        if loss < self.best_loss:
            self.best_loss = loss
        return step, checkpoint.get("loss_history", [])
