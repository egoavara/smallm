# %% [markdown]
# # SmallM - Model Training
#
# LLaMA ìŠ¤íƒ€ì¼ ì–¸ì–´ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
#
# **ì‚¬ì „ ìš”êµ¬ì‚¬í•­**: train-tokenizer.pyë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ë¨¼ì € í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤.

# %% [markdown]
# ## 1. Setup & Imports

# %%
import os
import torch
from pathlib import Path
from IPython.display import display, clear_output
import ipywidgets as widgets
from typing import Optional
import time

from smallm.model import LLaMA, CONFIGS
from smallm.data import (
    load_dataset_by_name,
    load_mixed_dataset,
    create_dataloader,
    load_streaming_dataset,
    load_streaming_mixed_dataset,
    create_streaming_dataloader,
)
from smallm.training import CheckpointManager, TrainingUI
from config import config

# configì—ì„œ ì„¤ì •ëœ BPE í´ë˜ìŠ¤ ì‚¬ìš©
BPETokenizer = config.tokenizer.get_bpe_class()

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# %% [markdown]
# ## 2. Configuration

# %%
print("=== Model Configuration ===")
print(f"  model_size: {config.model.model_size}")
print(f"  seq_len: {config.model.seq_len}")
print(f"  batch_size: {config.model.batch_size}")
print(f"  learning_rate: {config.model.learning_rate}")
print(f"  device: {config.model.device}")
print(f"  tokenizer_path: {config.tokenizer_path}")
print(f"  save_best: {config.model.save_best}")
print(f"  max_checkpoints: {config.model.max_checkpoints}")
print(f"  auto_load_best: {config.model.auto_load_best}")

# %% [markdown]
# ## 3. Load Tokenizer

# %%
tokenizer_file = Path(f"{config.tokenizer_path}.model")
if not tokenizer_file.exists():
    raise FileNotFoundError(
        f"âŒ Tokenizer not found at {tokenizer_file}\n"
        f"   Please run train-tokenizer.py first!"
    )

tokenizer = BPETokenizer()
tokenizer.load(config.tokenizer_path)
print(f"âœ… Tokenizer loaded from {config.tokenizer_path}")
print(f"   Vocab size: {tokenizer.vocab_size}")

# %% [markdown]
# ## 4. Training State


# %%
class TrainingState:
    """í•™ìŠµ ìƒíƒœ ê´€ë¦¬ í´ë˜ìŠ¤."""

    def __init__(self):
        self.model: Optional[LLaMA] = None
        self.optimizer: Optional[torch.optim.AdamW] = None
        self.train_loader = None
        self.train_iter = None
        self.checkpoint_manager: Optional[CheckpointManager] = None
        self.ui: Optional[TrainingUI] = None

        self.step = 0
        self.loss_history = []

        self.is_training = False
        self.stop_requested = False

    def reset_iter(self):
        if self.train_loader:
            self.train_iter = iter(self.train_loader)

    def get_batch(self):
        try:
            return next(self.train_iter)
        except (StopIteration, TypeError):
            self.reset_iter()
            return next(self.train_iter)

    def save_checkpoint(self, loss: float) -> str:
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥."""
        return self.checkpoint_manager.save(
            step=self.step,
            loss=loss,
            model=self.model,
            optimizer=self.optimizer,
            loss_history=self.loss_history,
        )

    def load_best_checkpoint(self) -> bool:
        """best.pt ë¡œë“œ. ì„±ê³µ ì—¬ë¶€ ë°˜í™˜."""
        result = self.checkpoint_manager.load_best(self.model, self.optimizer)
        if result[0] is not None:
            self.step = result[0]
            self.loss_history = result[1]
            return True
        return False

    def load_checkpoint(self, path: str):
        """íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ."""
        self.step, self.loss_history = self.checkpoint_manager.load_checkpoint(
            path, self.model, self.optimizer
        )
        return self.step


state = TrainingState()
print("Training state initialized.")

# %% [markdown]
# ## 5. Model & Data Setup


# %%
def setup_model():
    """ëª¨ë¸ ì´ˆê¸°í™”."""
    model_config = CONFIGS[config.model.model_size]
    model_config.vocab_size = tokenizer.vocab_size
    model_config.max_seq_len = config.model.seq_len

    model = LLaMA(model_config).to(config.model.device)

    print(f"\nğŸ“¦ Model: {config.model.model_size}")
    print(f"   Parameters: {model.count_parameters():,}")

    state.model = model
    state.optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.model.learning_rate,
        weight_decay=config.model.weight_decay,
    )

    # CheckpointManager ì´ˆê¸°í™” (model_sizeë¥¼ ê²½ë¡œì— í¬í•¨)
    checkpoint_dir = f"{config.model.checkpoint_dir}/{config.model.model_size}"
    state.checkpoint_manager = CheckpointManager(
        checkpoint_dir=checkpoint_dir,
        max_checkpoints=config.model.max_checkpoints,
        save_best=config.model.save_best,
        device=config.model.device,
    )

    # auto_load_bestê°€ Trueì´ë©´ best.pt ìë™ ë¡œë“œ ì‹œë„
    if config.model.auto_load_best:
        if state.load_best_checkpoint():
            print(f"   Resuming from step {state.step}")
        else:
            print("   No best.pt found, starting fresh")

    return model


def setup_data(split: str = "train"):
    """ë°ì´í„° ë¡œë” ì„¤ì •."""
    is_streaming = config.dataset.streaming

    if is_streaming:
        # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì , ì…”í”Œ ë²„í¼ë¡œ ëœë¤ì„± ë³´ì¥)
        if config.dataset.sources:
            dataset = load_streaming_mixed_dataset(
                sources=config.dataset.sources,
                tokenizer=tokenizer,
                split=split,
                seq_len=config.model.seq_len,
                buffer_size=config.dataset.buffer_size,
                shuffle_buffer_size=config.dataset.shuffle_buffer_size,
            )
            dataset_name = "Mixed (streaming)"
        else:
            dataset = load_streaming_dataset(
                name=config.dataset.name,
                tokenizer=tokenizer,
                split=split,
                seq_len=config.model.seq_len,
                buffer_size=config.dataset.buffer_size,
                shuffle_buffer_size=config.dataset.shuffle_buffer_size,
            )
            dataset_name = f"{config.dataset.name} (streaming)"

        state.train_loader = create_streaming_dataloader(
            dataset,
            batch_size=config.model.batch_size,
        )
    else:
        # ì¸ë©”ëª¨ë¦¬ ëª¨ë“œ (ê¸°ì¡´ ë°©ì‹)
        if config.dataset.sources:
            dataset = load_mixed_dataset(
                sources=config.dataset.sources,
                tokenizer=tokenizer,
                split=split,
                seq_len=config.model.seq_len,
            )
            dataset_name = "Mixed"
        else:
            dataset = load_dataset_by_name(
                name=config.dataset.name,
                tokenizer=tokenizer,
                split=split,
                seq_len=config.model.seq_len,
                max_samples=config.dataset.max_samples,
            )
            dataset_name = config.dataset.name

        state.train_loader = create_dataloader(
            dataset,
            batch_size=config.model.batch_size,
            shuffle=True,
        )

    state.reset_iter()

    print(f"\nğŸ“Š Dataset ({dataset_name}): {len(dataset):,} samples")
    print(f"   Batch size: {config.model.batch_size}")
    if not is_streaming:
        print(f"   Steps per epoch: {len(state.train_loader):,}")
    else:
        print("   Mode: Streaming (dynamic loading)")

    return state.train_loader


# %% [markdown]
# ## 6. Training Functions


# %%
def train_step() -> float:
    """ë‹¨ì¼ í•™ìŠµ ìŠ¤í…."""
    state.model.train()

    x, y = state.get_batch()
    x, y = x.to(config.model.device), y.to(config.model.device)

    state.optimizer.zero_grad()
    _, loss = state.model(x, y)
    loss.backward()

    torch.nn.utils.clip_grad_norm_(state.model.parameters(), config.model.max_grad_norm)
    state.optimizer.step()

    state.step += 1
    loss_val = loss.item()
    state.loss_history.append(loss_val)

    return loss_val


def train(
    num_steps: int = 1000,
    log_interval: int = 100,
    save_interval: int = 500,
    verbose: bool = True,
):
    """í•™ìŠµ ë£¨í”„."""
    if state.model is None:
        raise RuntimeError("Model not initialized. Call setup_model() first.")
    if state.train_loader is None:
        raise RuntimeError("Data not loaded. Call setup_data() first.")

    state.is_training = True
    state.stop_requested = False
    start_step = state.step
    start_time = time.time()

    print(f"\nğŸš€ Training for {num_steps} steps (from step {start_step})")
    print(f"   Log interval: {log_interval}, Save interval: {save_interval}")
    print("-" * 50)

    try:
        for _ in range(num_steps):
            if state.stop_requested:
                print("\nâ¹ï¸ Training stopped by user")
                break

            loss = train_step()

            if state.step % log_interval == 0:
                elapsed = time.time() - start_time
                steps_done = state.step - start_step
                steps_per_sec = steps_done / elapsed if elapsed > 0 else 0

                if verbose:
                    print(
                        f"Step {state.step:6d} | Loss: {loss:.4f} | "
                        f"Speed: {steps_per_sec:.1f} steps/s"
                    )

            if state.step % save_interval == 0:
                # ìµœê·¼ 100ìŠ¤í…ì˜ í‰ê·  loss ì‚¬ìš©
                recent_losses = state.loss_history[-100:]
                avg_loss = sum(recent_losses) / len(recent_losses)
                saved = state.save_checkpoint(avg_loss)
                if verbose:
                    print(f"   ğŸ’¾ Saved: {saved}")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ Training interrupted")

    finally:
        state.is_training = False
        elapsed = time.time() - start_time
        steps_done = state.step - start_step
        print("-" * 50)
        print(f"âœ… Completed {steps_done} steps in {elapsed:.1f}s")

        if state.loss_history:
            final_loss = state.loss_history[-1]
            print(f"   Final loss: {final_loss:.4f}")


# %% [markdown]
# ## 7. Generation


# %%
@torch.no_grad()
def generate(
    prompt: str = "",
    max_tokens: int = 100,
    temperature: float = 0.8,
    top_k: int = 50,
) -> str:
    """í…ìŠ¤íŠ¸ ìƒì„±."""
    if state.model is None:
        raise RuntimeError("Model not initialized.")

    state.model.eval()

    if prompt:
        tokens = tokenizer.encode(prompt)
    else:
        tokens = [tokenizer.bos_id] if hasattr(tokenizer, "bos_id") else [1]

    tokens = torch.tensor([tokens], device=config.model.device)

    for _ in range(max_tokens):
        logits, _ = state.model(tokens[:, -config.model.seq_len :])
        logits = logits[:, -1, :] / temperature

        if top_k > 0:
            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
            logits[logits < v[:, -1:]] = float("-inf")

        probs = torch.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)

        tokens = torch.cat([tokens, next_token], dim=1)

        if hasattr(tokenizer, "eos_id") and next_token.item() == tokenizer.eos_id:
            break

    return tokenizer.decode(tokens[0].tolist())


# %% [markdown]
# ## 8. Interactive UI (Jupyter)


# %%
def create_training_ui():
    """Jupyterìš© í•™ìŠµ UI ìƒì„± ë° í‘œì‹œ."""
    if state.model is None:
        raise RuntimeError("Model not initialized. Call setup_model() first.")
    if state.train_loader is None:
        raise RuntimeError("Data not loaded. Call setup_data() first.")

    def train_step_fn() -> float:
        """ë‹¨ì¼ í•™ìŠµ ìŠ¤í… (UIìš©)."""
        state.model.train()
        x, y = state.get_batch()
        x, y = x.to(config.model.device), y.to(config.model.device)

        state.optimizer.zero_grad()
        _, loss = state.model(x, y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(state.model.parameters(), config.model.max_grad_norm)
        state.optimizer.step()

        return loss.item()

    state.ui = TrainingUI(
        model=state.model,
        optimizer=state.optimizer,
        checkpoint_manager=state.checkpoint_manager,
        tokenizer=tokenizer,
        train_step_fn=train_step_fn,
        device=config.model.device,
        model_size=config.model.model_size,
    )

    # best.ptì—ì„œ ë¡œë“œëœ ê²½ìš° step ë™ê¸°í™”
    if state.step > 0:
        state.ui.set_step(state.step, state.loss_history)

    state.ui.display()


# %% [markdown]
# ## 9. Main Entry Point

# %%
def is_jupyter() -> bool:
    """Jupyter í™˜ê²½ì¸ì§€ í™•ì¸."""
    try:
        from IPython import get_ipython
        return get_ipython() is not None
    except ImportError:
        return False


if __name__ == "__main__":
    # ëª¨ë¸ ë° ë°ì´í„° ì„¤ì •
    setup_model()
    setup_data()

    # UI í‘œì‹œ
    create_training_ui()
