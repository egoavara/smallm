# %% [markdown]
# # SmallM - Tokenizer Training
#
# BPE í† í¬ë‚˜ì´ì € í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

# %% [markdown]
# ## 1. Setup

# %%
from pathlib import Path
from tqdm.auto import tqdm

from config import config
from smallm.data.registry import get_dataset_info
from smallm.data.loaders.base import load_hf_dataset, collect_texts

BPETokenizer = config.tokenizer.get_bpe_class()
print(f"Using {BPETokenizer.__name__} tokenizer")

# %% [markdown]
# ## 2. Configuration

# %%
print("=== Tokenizer Configuration ===")
print(f"  vocab_size: {config.tokenizer.vocab_size}")
print(f"  sample_size: {config.tokenizer.sample_size}")
print(f"  output_dir: {config.tokenizer.output_dir}")

# %% [markdown]
# ## 3. Load Data

# %%
# ë°ì´í„°ì…‹ ì´ë¦„ ê²°ì • (í˜¼í•© ëª¨ë“œë©´ ì²« ë²ˆì§¸ ì†ŒìŠ¤ ì‚¬ìš©)
dataset_name = (
    config.dataset.sources[0].name
    if config.dataset.sources
    else config.dataset.name
)

# ë°ì´í„°ì…‹ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
dataset_info = get_dataset_info(dataset_name)
mode_str = " (streaming)" if config.dataset.streaming else ""
print(f"\nLoading {dataset_info.description}{mode_str}...")

# HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_hf_dataset(
    dataset_info.hf_path,
    dataset_info.hf_subset,
    split=config.dataset.split,
    streaming=config.dataset.streaming,
)

# í…ìŠ¤íŠ¸ ìˆ˜ì§‘
full_text = collect_texts(
    dataset,
    text_column=dataset_info.text_column,
    max_samples=config.tokenizer.sample_size,
    desc="Collecting",
)

print(f"Total characters: {len(full_text):,}")

# %% [markdown]
# ## 4. Train Tokenizer

# %%
print(f"\nTraining BPE tokenizer (vocab_size={config.tokenizer.vocab_size})...")
print(f"Merges needed: {config.tokenizer.vocab_size - 256}")

tokenizer = BPETokenizer()
tokenizer.train(full_text, config.tokenizer.vocab_size, verbose=True)

# %% [markdown]
# ## 5. Add Special Tokens & Save

# %%
# Special tokens ë“±ë¡
special_tokens = {
    "<|endoftext|>": config.tokenizer.vocab_size,
    "<|pad|>": config.tokenizer.vocab_size + 1,
}
tokenizer.register_special_tokens(special_tokens)

# ì €ì¥ (í´ë˜ìŠ¤ëª…ìœ¼ë¡œ íŒŒì¼ êµ¬ë¶„)
output_dir = Path(config.tokenizer.output_dir)
output_dir.mkdir(parents=True, exist_ok=True)
tokenizer_name = BPETokenizer.__name__
save_path = output_dir / tokenizer_name
tokenizer.save(str(save_path))
print(f"\nâœ… Tokenizer saved to {save_path}.model")
print(f"   Final vocab size: {tokenizer.vocab_size}")

# %% [markdown]
# ## 6. Test Tokenizer

# %%
print("\n=== Tokenizer Test ===")
test_texts = [
    "Hello, world!",
    "This is a test of the BPE tokenizer.",
    "The quick brown fox jumps over the lazy dog.",
]

for text in test_texts:
    tokens = tokenizer.encode(text)
    decoded = tokenizer.decode(tokens)
    print(f"\nOriginal: {text!r}")
    print(f"Tokens ({len(tokens)}): {tokens[:15]}{'...' if len(tokens) > 15 else ''}")
    print(f"Decoded: {decoded!r}")
    print(f"Match: {'âœ…' if text == decoded else 'âŒ'}")

# %%
print("\nğŸ‰ Tokenizer training complete!")
print(f"   Now run train-model.py to train the model.")
